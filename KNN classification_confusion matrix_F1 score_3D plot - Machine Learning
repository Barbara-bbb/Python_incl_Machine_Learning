# Open https://jupyter.org/try-jupyter/lab/ using Google Chrome. Open Notebook with Python.
# In a new file, copy-paste the following code and run it. Pls wait a few seconds to get the output.

# Problem description: K-Nearest Neighbours supervised Machine Learning algorithm. 
# It is applied for prediction to which class a data point (data set array) belongs. 
import pandas as pd
import numpy as np
# This is done, based on comparison to majority of similar data points that belong to a class. 
# There are number of classes that contain similar to each other data points. 
# Number of classes is defined upfront, by number ‘k’.
# Accuracy evaluation by two methods (from the sklearn package): 
# 1) accuracy_score and 2) confusion_matrix / F1 score functions.


import matplotlib.pyplot as plt
%matplotlib inline

# Source data set
csv_path = 'data/iris.csv'
df = pd.read_csv(csv_path)
print(df.head())
print(df.tail())

# Get statistics of data frame
print(df.describe())

# Find unique species names
print('unique species:')
n = df['species'].unique()
print(n)

# Replacing values
df['species'].replace(['se', 'setosa','versicolor','virginica'],
                      [0, 1, 2, 3], inplace=True)
print('after replacement:')
print(df.head())

# Defining feature data set, X. 
# Converting Pandas data frame to Numpy array to use library sckit-learn

X = df[['sepal_length','sepal_width','petal_length','petal_width']].values.astype(float)
print('Numpy type dataset X:')
print(X[0:5])

# Labels data
y = df['species'].values
print('Numpy type dataset y:')
print(y[0:5])

# Normalize the data. Data Standardization with zero mean and unit variance.
from sklearn import preprocessing
X=preprocessing.StandardScaler().fit(X).transform(X.astype(float))
print('Normalized dataset X', X[0:5])

# Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
print('Training data set:', X_train.shape, y_train.shape)
print('Testing data set:', X_test.shape, y_test.shape)

print('X_train head:', X_train[0:5])
print('y_train head:',y_train[0:5])
print('X_test head:', X_test[0:15])
print('y_test head:',y_test[0:15])

# Importing library for KNN
from sklearn.neighbors import KNeighborsClassifier

#Training the KNN model 
k = 4
neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train, y_train)
neigh

# Prediction from the model using test data set
yhat = neigh.predict(X_test)
print('y_test_predicted head:', yhat[0:15])

# Evaluation of accuracy in terms of actual and predicted values
print('')
print('')
print('EVALUATION OF ACCURACY')
print('')
from sklearn import metrics
print('Accuracy of the Train data set:', metrics.accuracy_score(y_train, neigh.predict(X_train)))
print('Accuracy of the Test data set:', metrics.accuracy_score(y_test, yhat))

print('')
# Accuracy with the confusion matrix:
from sklearn.metrics import confusion_matrix, classification_report
cnf_matrix = confusion_matrix (y_test, yhat,labels=[0,1,2,3])
print('1. Test and pedicted accuracy using confusion matrix, for classes 0,1,2,3:')
print('Test population is 30 points = 20% out of 150')
print (cnf_matrix)
print('Comment: no class 0 in the test sample')
print('Classification report:')
print(classification_report(y_test, yhat))

print('y_test:', y_test)

print('')
cnf_matrix2 = confusion_matrix (y_train, neigh.predict(X_train),labels=[0,1,2,3])
print('2. Train and pedicted accuracy using confusion matrix, for classes 0,1,2,3:')
print('Train population is 120 points = 80% out of 150')
print(cnf_matrix2)
print('Classification report:')
print(classification_report(y_train, neigh.predict(X_train)))
print('Explanation of the UndefinedMetricWarning warning: only one sample of class 0 in the train sample; no predicted class 0.')
print('y_train', y_train)
print('y_train_predicted',neigh.predict(X_train))

# 3D plot: 
from mpl_toolkits import mplot3d
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

fig1 = plt.figure(1)
ax1 = plt.axes(projection='3d')
# Data for three-dimensional scattered points
xdata = X_test[:,0]
ydata = X_test[:,1]
zdata = y_test
ax1.scatter3D(xdata, ydata, zdata, c=zdata);
ax1.set_title ('Test dataset - Feature 1 & 2')
ax1.set_xlabel('sepal_length')
ax1.set_ylabel('sepal_width')
ax1.set_zlabel('species')
#

fig2 = plt.figure(2)
ax2 = plt.axes(projection='3d')
# Data for three-dimensional scattered points
xdata = X_test[:,0]
ydata = X_test[:,1]
zdata = yhat
ax2.scatter3D(xdata, ydata, zdata, c=zdata);
ax2.set_title ('Predicted test dataset - Feature 1 & 2')
ax2.set_xlabel('sepal_length')
ax2.set_ylabel('sepal_width')
ax2.set_zlabel('species')

#
fig3 = plt.figure(3)
ax3 = plt.axes(projection='3d')
# Data for three-dimensional scattered points
xdata = X_test[:,2]
ydata = X_test[:,3]
zdata = y_test
ax3.scatter3D(xdata, ydata, zdata, c=zdata);
ax3.set_title ('Test dataset - Feature 3 & 4')
ax3.set_xlabel('petal_length')
ax3.set_ylabel('petal_width')
ax3.set_zlabel('species')
#

fig4 = plt.figure(4)
ax4 = plt.axes(projection='3d')
# Data for three-dimensional scattered points
xdata = X_test[:,2]
ydata = X_test[:,3]
zdata = yhat
ax4.scatter3D(xdata, ydata, zdata, c=zdata);
ax4.set_title ('Predicted test dataset - Feature 3 & 4')
ax4.set_xlabel('petal_length')
ax4.set_ylabel('petal_width')
ax4.set_zlabel('species')
