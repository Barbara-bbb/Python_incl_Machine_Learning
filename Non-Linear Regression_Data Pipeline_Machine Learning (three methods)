# Open https://jupyter.org/try-jupyter/lab/ using Google Chrome. Open Notebook with Python.
# In a new file, copy-paste the following code and run it. Pls wait a few seconds to get the output.

# THE CODE

print('____________________________________________________________________________________________')
print('')
print('Problem description: non-linear regression analysis in Python, using three methods: ')
print('1) Mathematical , 2) Polynomial Regression (function .polyfit), 3) Data Pipeline')
print('for verification, utilizing Machine Learning.')
print('')
print('___________________________________________________________________________________________')


# 1. Non-linear regression example

print('1. Non-linear regression in Python:')
# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pylab as plt

# Plotting
x = np.arange (-10.0, 10.0, 0.8)
# ** is the power operator
y = 10 * x +  x ** 2 
y_noise = 10 * np.random.normal (size = x.size)
y_data = y + y_noise

plt.plot(x, y_data, 'bo')
plt.plot(x, y,'r')
plt.title ('Non-linear regression / Ploynomial Fit in Python')
plt.xlabel ('Independent variable')
plt.ylabel ('Dependent variable')
plt.show()

# Accuracy of model
print("Mean absolute error: %.2f" % np.mean(np.absolute(y - y_data)))
print("Residual sum of squares (MSE): %.2f" % np.mean((y - y_data) ** 2))
# Ideal MSE = 0. Interpret MSE: https://stephenallwright.com/interpret-mse/

from sklearn.metrics import r2_score 
print("R2-score: %.2f" % r2_score(y_data,y))
# Ideal R2 = 1.


# 2. The same example as above, solved using Polynomial Regression (function .polyfit) 
# for verification

print('')
print('2. Problem solved using Polynomial Regression (function .polyfit) for verification:')
print('')

f=np.polyfit (x, y_data, 2)
p=np.poly1d(f)   # polynomial equation
print('Equation of predicted model:')
print(p)


def PlotPolly (model, independent_variable, dependent_variable, Name):
    x_new = np.arange (-10.0, 10.0, 0.8)
    y_new = model (x_new)
    
    plt.plot(independent_variable, dependent_variable,'.',x_new,y_new,'-')
    plt.title('Non-linear regression / Polynomial Fit with Matplotlib')
    ax=plt.gca()
    ax.set_facecolor((0.898, 0.898, 0.898))
    fig=plt.gcf()
    plt.xlabel(Name)
    plt.ylabel('Dependent variable y')
    plt.show()
    plt.close()

PlotPolly(p, x, y_data, 'Independent variable x')
np.polyfit(x,y_data,2)


# 3. The same as in the two examples above, solved using Pipelines (Polynomial Regression) 
# for verification; 1-dimensional regression: only one variable data set example.

print('')
print (' 3.  The same example as above, solved using Pipelines')
print('')
       
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Pipeline Constructor, estimator model is polynomial, model constructor is LinearRegression
Input = [('scale', StandardScaler()),('polynomial', PolynomialFeatures(degree = 2)), 
         ('model', LinearRegression())]
pipe = Pipeline(Input)
pipe

# Convert Numpy to Pandas data frame. Pandas data frame are used for the Pipeline.
x = pd.DataFrame(x)
Z = x
y_data = pd.DataFrame(y_data)

# Training the pipeline object. 
Z = Z.astype(float)
pipe.fit (Z, y_data)

# Normalize the data, perform transform and create prediction at the same time
y_pred_pipe = pipe.predict (Z)
y_pred_pipe[0:4] 

# Plotting
plt.plot(Z, y_pred_pipe, 'r')
plt.plot(Z, y_data,'bo')
plt.title ('Non-linear regression / Polynomial Fit with Pipeline')
plt.xlabel ('Independent variable')
plt.ylabel ('Dependent variable')
plt.show()

# Accuracy of model
print("Mean absolute error: %.2f" % np.mean(np.absolute(y_pred_pipe - y_data)))
print("Residual sum of squares (MSE): %.2f" % np.mean((y_pred_pipe - y_data) ** 2))

print("R2-score: %.2f" % r2_score(y_pred_pipe, y_data))
# Ideal R2 = 1.
