# Open https://jupyter.org/try-jupyter/lab/ using Google Chrome. Open Notebook with Python.
# In a new file, copy-paste the following code and run it. Pls wait a few seconds to get the output.

# THE CODE
print('_______________________________________________________________________________________')
print('')
print('Problem description: Logistic Regression classification Machine Learning algorithm. ')
print('It is applied for prediction to which class a data point (data set array) belongs,')
print('on the basis of the Probability of the Class Estimates.')
print('')
print('This is done, based on comparison to majority of similar data points that belong to a class.')
print('There are number of classes that contain similar to each other data points. ')
print('')
print('Accuracy evaluation by two methods (from the sklearn package): ')
print(' 1) confusion_matrix / F1 score and 2) log loss (Logarithmic loss).')
print('')
print('Log loss measures performance of a classifier with a chosen solver, where the predicted')
print('probability output is between 0 and 1.')
print('Logistic Regression  solvers of choice for model training are: liblinear, newton-cg')
print('lbfgs, sag, saga.')
print('')
print('C is the parameter that indicates inverse of regularization strength (positive float).')
print('Smaller C value specify stronger regularization.')
print('')
print('_______________________________________________________________________________________')


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

# Source data set
csv_path = 'data/iris.csv'
df = pd.read_csv(csv_path)
print('')
print(df.head())
print(df.tail())

# Get statistics of data frame
print('')
print(df.describe())

# Find unique species names
print('')
print('unique species:')
n = df['species'].unique()
print(n)

# Replacing categorical variable into values 
df['species'].replace(['se', 'setosa','versicolor','virginica'],
                      [0, 1, 2, 3], inplace=True)
# Deleting one row of 'se' category - only one point in data set, for the sake of having uniform data set
df = df.drop(0, axis=0)
print('')
print('after replacement:')
print(df.head())
print('')
print('Numer of rows:', len(df))

# Defining feature data set - the feature matrix, X. 
# Converting Pandas data frame to Numpy array to use library scikit-learn

X = df[['sepal_length','sepal_width','petal_length','petal_width']].values.astype(float)
print('')
print('Numpy array X / the feature matrix X:')
print( X[0:5])

# Labels data - the response vector y.
y = df['species'].values
print('')
print('Numpy array y / the response vector y:')
print(y[0:5])

# Normalize the data. Data Standardization with zero mean and unit variance.
from sklearn import preprocessing
X=preprocessing.StandardScaler().fit(X).transform(X.astype(float))
print('')
print('Normalized X:')
print(X[0:5])

# Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
print('')
print('Training data set:', X_train.shape, y_train.shape)
print('')
print('Testing data set:', X_test.shape, y_test.shape)
print('')
print('X_train head:', X_train[0:5])
print('')
print('y_train head:',y_train[0:5])
print('')
print('X_test head:', X_test[0:5])
print('')
print('y_test head:',y_test[0:5])

# Importing library for Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

# Model training, solvers of choice are: 'liblinear','newton-cg', 'lbfgs','sag','saga'. 
LR = LogisticRegression (C=0.01, solver ='newton-cg').fit(X_train, y_train)
LR
# Prediction
yhat = LR.predict(X_test)
print('')
print('yhat: probability estimates / yhat')
print('of points belonging to each class:')
print(yhat)

# Prediction ordered by the classes labels
yhat_prob = LR.predict_proba(X_test)
print('')
print('yhat_prob: probability estimates / yhat of points')
print('belonging to each class, ordered by the classes lables: 1,2,3.')
print('Showing first 10 rows only:')
print(yhat_prob[0:10])

# 1. Accuracy of predictions with confusion matrix:
print('')
from sklearn.metrics import confusion_matrix, classification_report
cnf_matrix = confusion_matrix (y_test, yhat,labels=[1,2,3])
print('Test and pedicted accuracy using confusion matrix, for classes 1,2,3:')
print('Test population is 30 points = 20% out of 149')
print (cnf_matrix)
print('')
print('Class 1: TP_true_positive = 17 points, class 2: TP_true_positive = 4 points, class 3: TP_true_positive = 5 points')
print('Class 1: FP_false positive = 0 points, class 2: FP_false positive = 2 points, class 3: FP_false positive = 2 points')
print('Class 1: FN_false negative = 0 points, class 2: FN_false negative = 2 points, class 3: FN_false negative = 2 points')
print('')
print('Precision = TP / (TP + FP), Recall = TP / (TP + FN), F1 = 2 * (prc * rec) / (prc + rec)')
print('')

print('Classification report:')
print(classification_report(y_test, yhat))
print('y_test:', y_test)
print('')
print('Class 1: 17 points, class 2: 6 points, class 3: 7 points, total 30 test points')

# 2. Accuracy performance of classifier for predictions with log loss:
print('')
from sklearn.metrics import log_loss
print('log loss - performance of a classifier with the chosen solver is (between 0 and 1):')
print('')
print('Comment: classifier with lower log loss has better accuracy.')
print(log_loss(y_test, yhat_prob))

# Visualization
plt.figure(1)
plt.scatter(np.linspace(1,30,30), y_test)
plt.title('Test data points actual class')
plt.xlabel('Test data points index')
plt.ylabel('y - actual class')

plt.figure(2)
plt.scatter(np.linspace(1,30,30), yhat)
plt.title('Test data points predicted class')
plt.xlabel('Test data points index')
plt.ylabel('yhat - predicted class')

plt.figure(3)
print(yhat_prob[:,0].shape)
plt.scatter(np.linspace(1,30,30), yhat_prob[:,0])
# plt.legend(['Predicted pobability estimates - class 1'])
plt.title('Test data points predicted probability estimates for class 1')
plt.xlabel('Test data points index')
plt.ylabel('yhat_prob - predicted probability estimates for class 1')

plt.figure(4)
plt.scatter(np.linspace(1,30,30), yhat_prob[:,1])
plt.title('Test data points predicted probability estimates for class 2')
plt.xlabel('Test data points index')
plt.ylabel('yhat_prob - predicted probability estimates for class 2')

plt.figure(5)
plt.scatter(np.linspace(1,30,30), yhat_prob[:,2])
plt.title('Test data points predicted probability estimates for class 3')
plt.xlabel('Test data points index')
plt.ylabel('yhat_prob - predicted probability estimates for class 3')

