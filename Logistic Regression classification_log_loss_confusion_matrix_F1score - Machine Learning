# Open https://jupyter.org/try-jupyter/lab/ using Google Chrome. Open Notebook with Python.
# In a new file, copy-paste the following code and run it. Pls wait a few seconds to get the output.

# Problem description: Logistic Regression classification Machine Learning algorithm. 
# It is applied for prediction to which class a data point (data set array) belongs, 
# on the basis of the Probability of the Class Estimates.

# This is done, based on comparison to majority of similar data points that belong to a class. 
# There are number of classes that contain similar to each other data points. 

# Accuracy evaluation by two methods (from the sklearn package): 
# 1) confusion_matrix / F1 score and 2) log loss (Logarithmic loss).

# Log loss measures performance of a classifier with a chosen solver, where the predicted probability output is between 0 and 1.
# Logistic Regression  solvers of choice for model training are: 'liblinear','newton-cg', 'lbfgs','sag','saga'. 

# C is the parameter that indicates inverse of regularization strength (positive float).
# Smaller C value specify stronger regularization.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

# Source data set
csv_path = 'data/iris.csv'
df = pd.read_csv(csv_path)
print(df.head())
print(df.tail())

# Get statistics of data frame
print(df.describe())

# Find unique species names
print('unique species:')
n = df['species'].unique()
print(n)

# Replacing categorical variable into values 
df['species'].replace(['se', 'setosa','versicolor','virginica'],
                      [0, 1, 2, 3], inplace=True)
# Deleting one row of 'se' category - only one point in data set, for the sake of having uniform data set
df = df.drop(0, axis=0)
print('after replacement:')
print(df.head())
print(df.describe)

# Defining feature data set, X. 
# Converting Pandas data frame to Numpy array to use library sckit-learn

X = df[['sepal_length','sepal_width','petal_length','petal_width']].values.astype(float)
print(X[0:5])

# Labels data
y = df['species'].values
print(y[0:5])

# Normalize the data. Data Standardization with zero mean and unit variance.
from sklearn import preprocessing
X=preprocessing.StandardScaler().fit(X).transform(X.astype(float))
print(X[0:5])

# Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
print('Training data set:', X_train.shape, y_train.shape)
print('Testing data set:', X_test.shape, y_test.shape)

print('X_train head:', X_train[0:5])
print('y_train head:',y_train[0:5])
print('X_test head:', X_test[0:5])
print('y_test head:',y_test[0:5])

# Importing library for Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

# Model training, solvers of choice are: 'liblinear','newton-cg', 'lbfgs','sag','saga'. 
LR = LogisticRegression (C=0.01, solver ='newton-cg').fit(X_train, y_train)
LR
# Prediction
yhat = LR.predict(X_test)
print('yhat: probability estimates / yhat of points belonging to each class',yhat)

# Prediction ordered by the classes labels
yhat_prob = LR.predict_proba(X_test)
print('yhat_prob: probability estimates / yhat of points belonging to each class, ordered by the classes lables: 1,2,3:')
print(yhat_prob)

# 1. Accuracy of predictions with confusion matrix:
print('')
from sklearn.metrics import confusion_matrix, classification_report
cnf_matrix = confusion_matrix (y_test, yhat,labels=[1,2,3])
print('Test and pedicted accuracy using confusion matrix, for classes 1,2,3:')
print('Test population is 30 points = 20% out of 149')
print (cnf_matrix)
print('')
print('Classification report:')
print(classification_report(y_test, yhat))
print('y_test:', y_test)

# 2. Accuracy performance of classifier for predictions with log loss:
print('')
from sklearn.metrics import log_loss
print('log loss - performance of a classifier with the chosen solver is (between 0 and 1):')
print('Comment: classifier with lower log loss has better accuracy.')
print(log_loss(y_test, yhat_prob))

# Visualization
plt.figure(1)
plt.scatter(X_test[:,0], y_test)
# plt.legend(['Normalized feature 1 data' ])
plt.title('Test data points for Feature 1 with actual class')
plt.xlabel('X_test normalized - Feature 1')
plt.ylabel('y - actual class')

plt.figure(2)
plt.scatter(X_test[:,0], yhat)
# plt.legend(['Predicted class'])
plt.title('Test data points for Feature 1 with class prediction')
plt.xlabel('X_test normalized - Feature 1')
plt.ylabel('yhat - predicted class')

plt.figure(3)
plt.scatter(X_test[:,0], yhat_prob[:,0])
# plt.legend(['Predicted class'])
plt.title('Test data points for Feature 1 with predicted probability estimates for class 1')
plt.xlabel('X_test normalized - Feature 1')
plt.ylabel('yhat_prob - predicted probability estimates for class 1')
